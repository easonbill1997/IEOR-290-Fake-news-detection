# -*- coding: utf-8 -*-
"""290.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/181gjUCsVdaBXJ7dfW9Ns9MedGvdOJY9V
"""

import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
import spacy
import string
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
from sklearn.pipeline import Pipeline
import numpy as np
from sklearn.model_selection import train_test_split
import re
from nltk.stem import PorterStemmer
import tensorflow as tf
import tensorflow_hub as hub
from google.colab import drive
import os


pd.set_option('display.max_colwidth',1000)
train = pd.read_csv('train.tsv', sep='\t', header = None)
train.columns = ['ID', 'label', 'statement','subject','speaker','job title','state', 'party','a','b','c','d','e','occasion']
train_new = train.drop(['ID','a','b','c','d','e'],axis = 1)

train_new.head()

#first split the subject
from keras.preprocessing.sequence import pad_sequences
list_subject = train_new['subject']
list_subject = [str(item).split(',') for item in list_subject]
for i in range(len(list_subject)):
    if len(list_subject[i])<5:
        j = len(list_subject[i])
        while j < 5:
            j+=1
            list_subject[i].append(0)
    else: 
        list_subject[i] = list_subject[i][:5]
list_subject = np.array(list_subject).T

train_new['subject_1'] = list_subject[0]
train_new['subject_2'] = list_subject[1]
train_new['subject_3'] = list_subject[2]
train_new['subject_4'] = list_subject[3]
train_new['subject_5'] = list_subject[4]

train_new.head()



#deal with job title
#to word embedding

data_statement=train_new['statement']
data_subject=train_new['subject']
def lemma(msg):
    lemmatizer = WordNetLemmatizer()
    word_list = nltk.word_tokenize(msg)
    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
    return lemmatized_output
def clean_text(text):
    text = text.replace("\n"," ")
    text = text.replace("\t"," ")
    text = text.replace("-", ' ')
    text = text.lower()
    return text

train_new['statement_new'] = train_new['statement'].astype(str).str.replace('[^\w\s]','').apply(clean_text)
train_new['statement_new'] = train_new['statement_new'].astype(str).apply(lemma)
# train_new['subject'] = train_new['subject'].apply(lemma)

train_new.head()

body = train_new.statement_new
bow = {}
for sentence in body:
    words = sentence.split(' ')
    for word in words:
        if not word in bow:
            bow[word] = 1
        else: bow[word] += 1
nbow = {}
i = 0
for item in bow:
    if bow[item] > 5: 
        nbow[item] = i
        i += 1

bow_list = []
for sentence in body:
    temp_bow = [0 for i in range(len(nbow))]
    words = sentence.split(' ')
    for word in words:
        if word in nbow:
            temp_bow[nbow[word]] += 1
    bow_list.append(np.array(temp_bow))

def sentence2bow(sentence, bow):
    sentence_bow = [0 for i in range(len(bow))]
    words = sentence.split(' ')
    for word in words:
        if word in nbow:
            sentence_bow[nbow[word]] += 1
    return sentence_bow

import pickle
output = open('dict.pkl', 'wb')

pickle.dump(nbow, output)

inp = open('dict.pkl','rb')
d = pickle.load(inp)

d

train_new['bow'] = bow_list

train_new.head()

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
def embed_sentences(sentence):
  sentence = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', sentence)
  sentence = tf.convert_to_tensor(sentence)
  sentence = embed(sentence)
  return np.array(sentence[0])

train_new['embed'] = train_new.statement_new.apply(embed_sentences)

train_new.head()

y = train_new.label
x = np.array(train_new.embed)

#further process on y
d = {'barely-true':1, 'false':0, 'mostly-true':1,'pants-fire':0,'true':1, 'half-true':1}
y_binary = [d[_] for _ in y]

#construct lr model
from sklearn import linear_model
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)
x_train = [_.reshape(512) for _ in x_train]
x_test = [_.reshape(512) for _ in x_test]

x_train_bin, x_test_bin, y_train_bin, y_test_bin = train_test_split(x, y_binary, test_size=0.2, random_state=100)
x_train_bin = [_.reshape(512) for _ in x_train_bin]
x_test_bin = [_.reshape(512) for _ in x_test_bin]

lr = linear_model.LogisticRegression(solver = 'newton-cg', multi_class='multinomial')
lr.fit(x_train, y_train)
train_acc=lr.score(x_train,y_train)
print ('Training Accuracy:',train_acc)

test_acc=lr.score(x_test,y_test)
print('Accuracy of the model on unseen test data: ',test_acc)

lr = linear_model.LogisticRegression(solver = 'newton-cg', multi_class='multinomial')
lr.fit(x_train_bin, y_train_bin)
train_acc=lr.score(x_train_bin,y_train_bin)
print ('Training Accuracy:',train_acc)

test_acc=lr.score(x_test_bin,y_test_bin)
print('Accuracy of the model on unseen test data: ',test_acc)

from sklearn.externals import joblib
joblib.dump(lr,'lr.joblib')
